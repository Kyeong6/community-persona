"""
FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ìŠ¤í¬ë¦½íŠ¸
CSV íŒŒì¼ì˜ ë°ì´í„°ë¥¼ FAISS ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ì‹¤í–‰
    python load_faiss_data.py
    
    # ë‹¤ë¥¸ CSV íŒŒì¼ ì‚¬ìš©
    python load_faiss_data.py --csv my_data.csv
    
    # IVF ì¸ë±ìŠ¤ íƒ€ì… ì‚¬ìš© (ë¹ ë¥¸ ê·¼ì‚¬ ê²€ìƒ‰)
    python load_faiss_data.py --index-type ivf
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°
    python load_faiss_data.py --no-test
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜ ì¡°ì •
    python load_faiss_data.py --test-queries 5

í•„ìš”í•œ ì»¬ëŸ¼:
    - id: int (ê³ ìœ  ì‹ë³„ì)
    - channel: str (ì±„ë„ëª…)
    - category: str (ì¹´í…Œê³ ë¦¬)
    - title: str (ì œëª©)
    - content: str (ë‚´ìš©)
    - view_cnt: int (ì¡°íšŒìˆ˜)
    - like_cnt: int (ì¢‹ì•„ìš” ìˆ˜)
    - comment_cnt: int (ëŒ“ê¸€ ìˆ˜)
    - created_at: datetime (ìƒì„±ì¼ì‹œ)
    - vector_title: str (ì œëª© ì„ë² ë”© ë²¡í„°, ì˜ˆ: "[0.1, 0.2, 0.3, ...]")
    - vector_content: str (ë‚´ìš© ì„ë² ë”© ë²¡í„°, ì˜ˆ: "[0.1, 0.2, 0.3, ...]")

ì˜ì¡´ì„± ì„¤ì¹˜:
    poetry install --no-root

ì¶œë ¥ íŒŒì¼:
    - data/faiss_indices/title_index_YYYYMMDD_HHMMSS.faiss (ì œëª© ë²¡í„° ì¸ë±ìŠ¤)
    - data/faiss_indices/content_index_YYYYMMDD_HHMMSS.faiss (ë‚´ìš© ë²¡í„° ì¸ë±ìŠ¤)
    - data/faiss_indices/metadata_YYYYMMDD_HHMMSS.pkl (ë©”íƒ€ë°ì´í„°)
    - data/faiss_indices/config_YYYYMMDD_HHMMSS.json (ì„¤ì • ì •ë³´)
"""

import pandas as pd
import numpy as np
import faiss
import pickle
import json
from datetime import datetime
import os
import sys
from pathlib import Path
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent
DATA_DIR = PROJECT_ROOT / "data"
INDEX_DIR = DATA_DIR / "faiss_indices"

# ë””ë ‰í† ë¦¬ ìƒì„±
DATA_DIR.mkdir(exist_ok=True)
INDEX_DIR.mkdir(exist_ok=True)

def load_csv_data(csv_path):
    """CSV íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    print(f"ğŸ“ CSV íŒŒì¼ ë¡œë“œ ì¤‘: {csv_path}")
    
    try:
        # CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv(csv_path)
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰")
        
        # ì»¬ëŸ¼ í™•ì¸
        expected_columns = [
            'id', 'channel', 'category', 'title', 'content', 
            'view_cnt', 'like_cnt', 'comment_cnt', 'created_at',
            'vector_title', 'vector_content'
        ]
        
        missing_columns = [col for col in expected_columns if col not in df.columns]
        if missing_columns:
            print(f"âŒ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns}")
            return None
            
        print(f"ğŸ“Š ë°ì´í„° ì •ë³´:")
        print(f"   - ì´ í–‰ ìˆ˜: {len(df)}")
        print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"   - ì±„ë„ ì¢…ë¥˜: {df['channel'].nunique()}ê°œ")
        print(f"   - ì¹´í…Œê³ ë¦¬ ì¢…ë¥˜: {df['category'].nunique()}ê°œ")
        
        return df
        
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def parse_vector_string(vector_str):
    """ë²¡í„° ë¬¸ìì—´ì„ numpy ë°°ì—´ë¡œ ë³€í™˜"""
    try:
        if pd.isna(vector_str) or vector_str == '':
            return None
            
        # ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(vector_str, str):
            # ëŒ€ê´„í˜¸ ì œê±° ë° ì‰¼í‘œë¡œ ë¶„ë¦¬
            vector_str = vector_str.strip('[]')
            vector_list = [float(x.strip()) for x in vector_str.split(',')]
            return np.array(vector_list, dtype=np.float32)
        else:
            return None
            
    except Exception as e:
        print(f"âš ï¸ ë²¡í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None

def create_faiss_indexes(df, index_type='flat'):
    """FAISS ì¸ë±ìŠ¤ ìƒì„±"""
    print("ğŸ”§ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    
    # ë²¡í„° ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬
    title_vectors = []
    content_vectors = []
    metadata = []
    
    valid_indices = []
    skipped_count = 0
    
    for idx, row in df.iterrows():
        # ë²¡í„° íŒŒì‹±
        title_vector = parse_vector_string(row['vector_title'])
        content_vector = parse_vector_string(row['vector_content'])
        
        if title_vector is not None and content_vector is not None:
            title_vectors.append(title_vector)
            content_vectors.append(content_vector)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata.append({
                'id': int(row['id']),
                'channel': str(row['channel']),
                'category': str(row['category']),
                'title': str(row['title']),
                'content': str(row['content']),
                'view_cnt': int(row['view_cnt']) if pd.notna(row['view_cnt']) else 0,
                'like_cnt': int(row['like_cnt']) if pd.notna(row['like_cnt']) else 0,
                'comment_cnt': int(row['comment_cnt']) if pd.notna(row['comment_cnt']) else 0,
                'created_at': str(row['created_at']),
                'original_index': idx
            })
            
            valid_indices.append(idx)
        else:
            skipped_count += 1
            if skipped_count <= 5:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                print(f"âš ï¸ í–‰ {idx} ë²¡í„° ë°ì´í„° ëˆ„ë½, ê±´ë„ˆëœ€")
    
    if skipped_count > 5:
        print(f"âš ï¸ ì´ {skipped_count}ê°œ í–‰ì˜ ë²¡í„° ë°ì´í„°ê°€ ëˆ„ë½ë˜ì–´ ê±´ë„ˆëœ€")
    
    if not title_vectors:
        print("âŒ ìœ íš¨í•œ ë²¡í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None, None
    
    # numpy ë°°ì—´ë¡œ ë³€í™˜
    title_vectors = np.array(title_vectors)
    content_vectors = np.array(content_vectors)
    
    print(f"âœ… ë²¡í„° ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
    print(f"   - ìœ íš¨í•œ ë°ì´í„°: {len(title_vectors)}ê°œ")
    print(f"   - ì œëª© ë²¡í„° ì°¨ì›: {title_vectors.shape[1]}")
    print(f"   - ë‚´ìš© ë²¡í„° ì°¨ì›: {content_vectors.shape[1]}")
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    title_index = create_optimized_index(title_vectors, index_type)
    content_index = create_optimized_index(content_vectors, index_type)
    
    print(f"âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ:")
    print(f"   - ì œëª© ì¸ë±ìŠ¤ í¬ê¸°: {title_index.ntotal}")
    print(f"   - ë‚´ìš© ì¸ë±ìŠ¤ í¬ê¸°: {content_index.ntotal}")
    
    return title_index, content_index, metadata

def save_indices_and_metadata(title_index, content_index, metadata):
    """ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
    print("ğŸ’¾ ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # FAISS ì¸ë±ìŠ¤ ì €ì¥
    title_index_path = INDEX_DIR / f"title_index_{timestamp}.faiss"
    content_index_path = INDEX_DIR / f"content_index_{timestamp}.faiss"
    
    faiss.write_index(title_index, str(title_index_path))
    faiss.write_index(content_index, str(content_index_path))
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_path = INDEX_DIR / f"metadata_{timestamp}.pkl"
    with open(metadata_path, 'wb') as f:
        pickle.dump(metadata, f)
    
    # ì„¤ì • ì •ë³´ ì €ì¥
    config = {
        'timestamp': timestamp,
        'total_records': len(metadata),
        'title_vector_dim': title_index.d,
        'content_vector_dim': content_index.d,
        'index_type': type(title_index).__name__,
        'created_at': datetime.now().isoformat(),
        'description': 'FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤'
    }
    
    config_path = INDEX_DIR / f"config_{timestamp}.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ:")
    print(f"   - ì œëª© ì¸ë±ìŠ¤: {title_index_path}")
    print(f"   - ë‚´ìš© ì¸ë±ìŠ¤: {content_index_path}")
    print(f"   - ë©”íƒ€ë°ì´í„°: {metadata_path}")
    print(f"   - ì„¤ì • íŒŒì¼: {config_path}")
    
    return {
        'title_index_path': str(title_index_path),
        'content_index_path': str(content_index_path),
        'metadata_path': str(metadata_path),
        'config_path': str(config_path),
        'timestamp': timestamp
    }

def test_index_search(title_index, content_index, metadata, test_queries=3):
    """ì¸ë±ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ì¸ë±ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    for i in range(min(test_queries, len(metadata))):
        # í…ŒìŠ¤íŠ¸ìš© ì¿¼ë¦¬ ë²¡í„° (ì²« ë²ˆì§¸ ë²¡í„° ì‚¬ìš©)
        title_query = title_index.reconstruct(i).reshape(1, -1)
        content_query = content_index.reconstruct(i).reshape(1, -1)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        title_distances, title_indices = title_index.search(title_query, k=5)
        content_distances, content_indices = content_index.search(content_query, k=5)
        
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ {i+1}:")
        print(f"   ì›ë³¸ ì œëª©: {metadata[i]['title'][:50]}...")
        print(f"   ì›ë³¸ ë‚´ìš©: {metadata[i]['content'][:50]}...")
        
        print(f"   ì œëª© ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼:")
        for j, (dist, idx) in enumerate(zip(title_distances[0], title_indices[0])):
            print(f"     {j+1}. ê±°ë¦¬: {dist:.4f}, ì œëª©: {metadata[idx]['title'][:30]}...")
        
        print(f"   ë‚´ìš© ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼:")
        for j, (dist, idx) in enumerate(zip(content_distances[0], content_indices[0])):
            print(f"     {j+1}. ê±°ë¦¬: {dist:.4f}, ë‚´ìš©: {metadata[idx]['content'][:30]}...")

def validate_csv_structure(df):
    """CSV êµ¬ì¡° ê²€ì¦"""
    print("ğŸ” CSV êµ¬ì¡° ê²€ì¦ ì¤‘...")
    
    required_columns = [
        'id', 'channel', 'category', 'title', 'content', 
        'view_cnt', 'like_cnt', 'comment_cnt', 'created_at',
        'vector_title', 'vector_content'
    ]
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"âŒ ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼: {missing_columns}")
        return False
    
    # ë°ì´í„° íƒ€ì… ê²€ì¦
    try:
        # ìˆ«ìí˜• ì»¬ëŸ¼ ê²€ì¦
        numeric_columns = ['id', 'view_cnt', 'like_cnt', 'comment_cnt']
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ë²¡í„° ì»¬ëŸ¼ ê²€ì¦
        vector_columns = ['vector_title', 'vector_content']
        for col in vector_columns:
            if df[col].isna().all():
                print(f"âŒ {col} ì»¬ëŸ¼ì— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
        
        print("âœ… CSV êµ¬ì¡° ê²€ì¦ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° íƒ€ì… ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

def create_optimized_index(vectors, index_type='flat'):
    """ìµœì í™”ëœ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
    print(f"ğŸ”§ {index_type} íƒ€ì… ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    
    if index_type == 'flat':
        # L2 ê±°ë¦¬ ê¸°ë°˜ í‰ë©´ ì¸ë±ìŠ¤ (ì •í™•í•œ ê²€ìƒ‰)
        index = faiss.IndexFlatL2(vectors.shape[1])
    elif index_type == 'ivf':
        # IVF ì¸ë±ìŠ¤ (ë¹ ë¥¸ ê·¼ì‚¬ ê²€ìƒ‰)
        quantizer = faiss.IndexFlatL2(vectors.shape[1])
        index = faiss.IndexIVFFlat(quantizer, vectors.shape[1], min(100, len(vectors)))
        index.train(vectors)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ íƒ€ì…: {index_type}")
    
    index.add(vectors)
    return index

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--csv', type=str, default='community_data.csv', 
                       help='CSV íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: community_data.csv)')
    parser.add_argument('--index-type', type=str, default='flat', 
                       choices=['flat', 'ivf'], help='FAISS ì¸ë±ìŠ¤ íƒ€ì… (ê¸°ë³¸ê°’: flat)')
    parser.add_argument('--test-queries', type=int, default=3, 
                       help='ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)')
    parser.add_argument('--no-test', action='store_true', 
                       help='ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    print("ğŸš€ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 50)
    print(f"ğŸ“ CSV íŒŒì¼: {args.csv}")
    print(f"ğŸ”§ ì¸ë±ìŠ¤ íƒ€ì…: {args.index_type}")
    print(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {args.test_queries}")
    print()
    
    # CSV íŒŒì¼ ê²½ë¡œ
    csv_path = PROJECT_ROOT / args.csv
    
    if not csv_path.exists():
        print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        print("ğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ CSV íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return 1
    
    try:
        # 1. CSV ë°ì´í„° ë¡œë“œ
        df = load_csv_data(csv_path)
        if df is None:
            return 1
        
        # 2. CSV êµ¬ì¡° ê²€ì¦
        if not validate_csv_structure(df):
            return 1
        
        # 3. FAISS ì¸ë±ìŠ¤ ìƒì„±
        title_index, content_index, metadata = create_faiss_indexes(df, args.index_type)
        if title_index is None:
            return 1
        
        # 4. ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥
        save_info = save_indices_and_metadata(title_index, content_index, metadata)
        
        # 5. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
        if not args.no_test:
            test_index_search(title_index, content_index, metadata, args.test_queries)
        
        print("\nğŸ‰ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì ì¬ ì™„ë£Œ!")
        print(f"ğŸ“ ì¸ë±ìŠ¤ íŒŒì¼ë“¤ì´ {INDEX_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìµœì‹  ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒì„±ëœ ì¸ë±ìŠ¤ ì •ë³´:")
        print(f"   - íƒ€ì„ìŠ¤íƒ¬í”„: {save_info['timestamp']}")
        print(f"   - ì´ ë ˆì½”ë“œ ìˆ˜: {len(metadata)}")
        print(f"   - ì œëª© ë²¡í„° ì°¨ì›: {title_index.d}")
        print(f"   - ë‚´ìš© ë²¡í„° ì°¨ì›: {content_index.d}")
        print(f"   - ì¸ë±ìŠ¤ íƒ€ì…: {args.index_type}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

